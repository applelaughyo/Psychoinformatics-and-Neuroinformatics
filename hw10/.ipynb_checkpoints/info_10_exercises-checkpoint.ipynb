{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Psychoinformatics - Week 10 (Examples)\n",
    "by your name (your email)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings, numpy as np\n",
    "import xgboost\n",
    "from matplotlib.pyplot import *\n",
    "%matplotlib inline\n",
    "warnings.simplefilter('ignore', DeprecationWarning)\n",
    "from sklearn import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 執行並觀察以下的機器學習結果 (2分)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.0 IRIS dataset & Ensemble model function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = datasets.load_iris()\n",
    "X=iris.data\n",
    "Y=iris.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "sss=model_selection.StratifiedShuffleSplit(n_splits=5,test_size=0.1)\n",
    "def EnsembleModels(model, Max_n_estimators):\n",
    "    accs=[] # mean cross-validation accuracies of the models w/ different n_estimators from 1..Max_n_estimators\n",
    "    for n in range(1,Max_n_estimators+1):  \n",
    "        print(n,end=' ') # showing progress\n",
    "        model.n_estimators=n\n",
    "        acc=[] # cross-validation accuracies of the ensemble model w/ n_estimators=n\n",
    "        for train_index, test_index in sss.split(X, Y): # 5-fold cross-validation of one ensemble model\n",
    "            X_train, X_test = X[train_index], X[test_index]\n",
    "            Y_train, Y_test = Y[train_index], Y[test_index]\n",
    "            model.fit(X_train[:,0:2],Y_train) #training\n",
    "            acc.append(model.predict(X_test[:,0:2])==Y_test) \n",
    "        accs.append(np.mean(acc)) # aggregating mean cross-validation accuracies across all the ensemble models\n",
    "    return(accs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Bagging (Bootstrap Aggregating)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.1 Tree max_depth = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model=ensemble.BaggingClassifier(tree.DecisionTreeClassifier(max_depth=1))\n",
    "plot(range(1,101),EnsembleModels(model,100));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.2 Tree max_depth = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=ensemble.BaggingClassifier(tree.DecisionTreeClassifier(max_depth=3))\n",
    "plot(range(1,101),EnsembleModels(model,100));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.1 AdaBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2.1.1 Tree max_depth = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=ensemble.AdaBoostClassifier(tree.DecisionTreeClassifier(max_depth=1))\n",
    "plot(range(1,101),EnsembleModels(model,100));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2.1.2 Tree max_depth = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=ensemble.AdaBoostClassifier(tree.DecisionTreeClassifier(max_depth=3))\n",
    "plot(range(1,101),EnsembleModels(model,100));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.2 Gradient Boosting\n",
    "\n",
    "The following two implementations are conceptually identical but XGBoost is more resource-efficient and can be parallelized/distributed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2.2.1 Scikit-learn's Gradient Tree Boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1.2.2.1.1 Tree max_depth = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=ensemble.GradientBoostingClassifier(max_depth=1)\n",
    "plot(range(1,101),EnsembleModels(model,100));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1.2.2.1.2 Tree max_depth = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=ensemble.GradientBoostingClassifier(max_depth=3)\n",
    "plot(range(1,101),EnsembleModels(model,100));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2.2.2 XGBoost (eXtreme Gradient Boosting)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1.2.2.2.1 Tree max_depth = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=xgboost.XGBClassifier(max_depth=1,eval_metric='mlogloss',use_label_encoder=False)\n",
    "plot(range(1,101),EnsembleModels(model,100));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1.2.2.2.2 Tree max_depth = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=xgboost.XGBClassifier(max_depth=3,eval_metric='mlogloss',use_label_encoder=False)\n",
    "plot(range(1,101),EnsembleModels(model,100));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 根據以上的觀察回答以下的問題 (6 分)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 在Bagging時, 1.1.2中複雜模型的正確率是否比1.1.1簡單模型的正確率好或差? 為什麼 (2分)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please write your answers here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 在Boosting時, 1.2.1.2/1.2.2.1.2/1.2.2.2.2中複雜模型的正確率是否比1.2.1.1/1.2.2.1.1/1.2.2.2.1中相對應的簡單模型正確率好或差? 為什麼 (2分)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please write your answers here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 為何只有Boosting在簡單模型時(1.2.1.1/1.2.2.1.1/1.2.2.2.1)，正確率大致上會隨著n_estimators數目變多而增加，但Bagging和複雜的Boosting模型卻不是如此? (2分)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please write your answers here."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
