{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zMpgme1iRLFc"
   },
   "source": [
    "# Psychoinformatics - Week 13 (Examples)\n",
    "by Tsung-Ren (Tren) Huang (trhuang@g.ntu.edu.tw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "zFR8ZcbPRLFe"
   },
   "outputs": [],
   "source": [
    "%config IPCompleter.greedy=True \n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "from matplotlib.pyplot import *\n",
    "from IPython.display import *\n",
    "import re\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RRjttjuQRLFg"
   },
   "source": [
    "## 1 Regular Expression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dYfFtS4fRLFh"
   },
   "source": [
    "### 1.0 Basic String Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "ff1YFbDWRLFh",
    "outputId": "96d943e6-951d-408c-fe81-698c77a16d51"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this is a built-in module THIS IS A BUILT-IN MODULE\n",
      "['This', 'is', 'a', 'built-in', 'module'] ['This is a built', 'in module']\n",
      "10 built\n",
      "This is a built-in library\n",
      "2\n",
      "============This is a built-in module=============\n"
     ]
    }
   ],
   "source": [
    "a='This is a built-in module'\n",
    "print(a.lower(),a.upper())\n",
    "print(a.split(' '),a.split('-'))\n",
    "print(a.find('built'),a[10:15])\n",
    "print(a.replace('module','library'))\n",
    "print(a.count('is'))\n",
    "print(a.center(50,'='))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5UKet3-PRLFj"
   },
   "source": [
    "### 1.1 Verification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "gjTVo9uARLFj",
    "outputId": "5cd7a2ab-9d0d-4ff7-981b-4bdd39d91b8e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "print(bool(re.match('[A-Za-z]\\d{9}','S123456789')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "Rt6jCX6ERLFk",
    "outputId": "e8cb5156-38d8-4435-a973-3536bee4b6e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "print(bool(re.match('[^@]+@[^@]+\\.[^@]+','a@b.c')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VdoeEqDXRLFk"
   },
   "source": [
    "### 1.2 Search, Match, & Find"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "Orb6UOX9RLFk",
    "outputId": "d44037d5-6bc1-4b4a-8820-e5db8b4d5e61"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "search() found: abc\n",
      "match() found: abc\n",
      "findall() found: ['abc']\n",
      "--------------------------------------------------\n",
      "search() found: hello abc\n",
      "findall() found: ['abc']\n",
      "--------------------------------------------------\n",
      "search() found: hi AbC aBc\n",
      "findall() found: ['AbC', 'aBc']\n"
     ]
    }
   ],
   "source": [
    "regex=re.compile('abc',re.IGNORECASE)\n",
    "for txt in ['abc','hello abc','hi AbC aBc']:\n",
    "    print('-'*50)\n",
    "    out=regex.search(txt) #inexact match\n",
    "    if(out): print('search() found:',out.string)\n",
    "    out=regex.match(txt) #exact match\n",
    "    if(out): print('match() found:',out.string)\n",
    "    out=regex.findall(txt) #search into a list\n",
    "    if(out): print('findall() found:',out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gO_CToenRLFl"
   },
   "source": [
    "### 1.3 Analysis of retweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "EH2HXfToRLFl",
    "outputId": "472c634e-df99-4409-a734-be1fe7c5f4a9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RT @spiketren RT @spiketren\n",
      "via @spiketren via @spiketren\n"
     ]
    }
   ],
   "source": [
    "tweets=['RT @spiketren  No class tomorrow','No class tomorrow (via @spiketren)']\n",
    "rt=re.compile('(RT|via) (@\\w+)')\n",
    "#rt=re.compile('(RT|\\(via) (@\\w+\\)*)')\n",
    "for t in tweets:\n",
    "    m=rt.search(t)\n",
    "    print(m.group(0),m.group(1),m.group(2));\n",
    "    #print(m[0],m[1],m[2]);\n",
    "    #print(m[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jmFCrUcWRLFm"
   },
   "source": [
    "### 1.4 Collection of email addresses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "1HnYEl7cRLFm",
    "outputId": "34d028eb-58d2-46d2-af07-a3e7c981d66b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test\n",
      "pichun_huang@mail.ncku.edu.tw,chendy@mail.ncku.edu.tw\n"
     ]
    }
   ],
   "source": [
    "html='<body><b>test</b><img src=test.jpg></body>'\n",
    "print(re.sub('<[^<]*>','',html))\n",
    "e='@mail.ncku.edu.tw'\n",
    "t='pichun_huang,chendy'\n",
    "print(re.sub('\\w+','\\g<0>'+e,t))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EgY1TytQRLFm"
   },
   "source": [
    "## 2 Traditional Natural Language Processing (NLP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yPGuGVNaRLFm"
   },
   "source": [
    "### 2.0 Download text corpora"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nvHdghjdRLFn"
   },
   "source": [
    "### 2.1 Lexical diversity & Big words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uCjISFFVRLFn",
    "outputId": "bf462b8f-836a-4922-9012-c62260b16382"
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "yrJGILF_RLFn",
    "outputId": "d7a3cfb6-b512-4af8-dc0a-d4336e0ac9dc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "a='cat'\n",
    "print(len(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "FA7j-vYyRLFo",
    "outputId": "c5086d14-9b87-4910-d381-615cf4c06805"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Introductory Examples for the NLTK Book ***\n",
      "Loading text1, ..., text9 and sent1, ..., sent9\n",
      "Type the name of the text or sentence to view it.\n",
      "Type: 'texts()' or 'sents()' to list the materials.\n",
      "text1: Moby Dick by Herman Melville 1851\n",
      "text2: Sense and Sensibility by Jane Austen 1811\n",
      "text3: The Book of Genesis\n",
      "text4: Inaugural Address Corpus\n",
      "text5: Chat Corpus\n"
     ]
    },
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mwebtext\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('webtext')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/webtext\u001b[0m\n\n  Searched in:\n    - '/Users/tren/nltk_data'\n    - '/Users/tren/opt/anaconda3/nltk_data'\n    - '/Users/tren/opt/anaconda3/share/nltk_data'\n    - '/Users/tren/opt/anaconda3/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/nltk/corpus/util.py\u001b[0m in \u001b[0;36m__load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     82\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 83\u001b[0;31m                     \u001b[0mroot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"{}/{}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubdir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mzip_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     84\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/nltk/data.py\u001b[0m in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    582\u001b[0m     \u001b[0mresource_not_found\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"\\n%s\\n%s\\n%s\\n\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0msep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 583\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresource_not_found\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    584\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mwebtext\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('webtext')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/webtext.zip/webtext/\u001b[0m\n\n  Searched in:\n    - '/Users/tren/nltk_data'\n    - '/Users/tren/opt/anaconda3/nltk_data'\n    - '/Users/tren/opt/anaconda3/share/nltk_data'\n    - '/Users/tren/opt/anaconda3/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-10318edeb0ef>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbook\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtext4\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# lexical diversity\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mlong_words\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mw\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext4\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m>\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlong_words\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# number of big words\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/nltk/book.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"text5:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext5\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m \u001b[0mtext6\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mText\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwebtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"grail.txt\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"Monty Python and the Holy Grail\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"text6:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext6\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/nltk/corpus/util.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, attr)\u001b[0m\n\u001b[1;32m    118\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"LazyCorpusLoader object has no attribute '__bases__'\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 120\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    121\u001b[0m         \u001b[0;31m# This looks circular, but its not, since __load() changes our\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m         \u001b[0;31m# __class__ to something new:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/nltk/corpus/util.py\u001b[0m in \u001b[0;36m__load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     83\u001b[0m                     \u001b[0mroot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"{}/{}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubdir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mzip_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 85\u001b[0;31m                     \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     86\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m         \u001b[0;31m# Load the corpus.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/nltk/corpus/util.py\u001b[0m in \u001b[0;36m__load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     78\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 80\u001b[0;31m                 \u001b[0mroot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"{}/{}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubdir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     81\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/nltk/data.py\u001b[0m in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    581\u001b[0m     \u001b[0msep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"*\"\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m70\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    582\u001b[0m     \u001b[0mresource_not_found\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"\\n%s\\n%s\\n%s\\n\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0msep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 583\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresource_not_found\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    584\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mwebtext\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('webtext')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/webtext\u001b[0m\n\n  Searched in:\n    - '/Users/tren/nltk_data'\n    - '/Users/tren/opt/anaconda3/nltk_data'\n    - '/Users/tren/opt/anaconda3/share/nltk_data'\n    - '/Users/tren/opt/anaconda3/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "from nltk.book import text4 \n",
    "print(len(set(text4))/len(text4)) # lexical diversity\n",
    "long_words=[w for w in set(text4) if len(w)>10] \n",
    "print(len(long_words)) # number of big words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "angszVm8RLFo"
   },
   "source": [
    "### 2.2 Tokenization & Word distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "vjB_LVcZRLFp",
    "outputId": "49de7698-9611-4edc-980e-5d8bd2aea1e0"
   },
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mpunkt\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt/PY3/english.pickle\u001b[0m\n\n  Searched in:\n    - '/Users/tren/nltk_data'\n    - '/Users/tren/opt/anaconda3/nltk_data'\n    - '/Users/tren/opt/anaconda3/share/nltk_data'\n    - '/Users/tren/opt/anaconda3/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n    - ''\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-9a990b530ddc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mmytxt\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'This is a cat. That is a dog.'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msent_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmytxt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mText\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmytxt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/nltk/tokenize/__init__.py\u001b[0m in \u001b[0;36msent_tokenize\u001b[0;34m(text, language)\u001b[0m\n\u001b[1;32m    105\u001b[0m     \u001b[0;34m:\u001b[0m\u001b[0mparam\u001b[0m \u001b[0mlanguage\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mPunkt\u001b[0m \u001b[0mcorpus\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m     \"\"\"\n\u001b[0;32m--> 107\u001b[0;31m     \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"tokenizers/punkt/{0}.pickle\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlanguage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    108\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/nltk/data.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(resource_url, format, cache, verbose, logic_parser, fstruct_reader, encoding)\u001b[0m\n\u001b[1;32m    748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    749\u001b[0m     \u001b[0;31m# Load the resource.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 750\u001b[0;31m     \u001b[0mopened_resource\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_open\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresource_url\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    752\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mformat\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"raw\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/nltk/data.py\u001b[0m in \u001b[0;36m_open\u001b[0;34m(resource_url)\u001b[0m\n\u001b[1;32m    873\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    874\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mprotocol\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"nltk\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 875\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    876\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"file\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    877\u001b[0m         \u001b[0;31m# urllib might not use mode='rb', so handle this one ourselves:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/nltk/data.py\u001b[0m in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    581\u001b[0m     \u001b[0msep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"*\"\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m70\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    582\u001b[0m     \u001b[0mresource_not_found\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"\\n%s\\n%s\\n%s\\n\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0msep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 583\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresource_not_found\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    584\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mpunkt\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt/PY3/english.pickle\u001b[0m\n\n  Searched in:\n    - '/Users/tren/nltk_data'\n    - '/Users/tren/opt/anaconda3/nltk_data'\n    - '/Users/tren/opt/anaconda3/share/nltk_data'\n    - '/Users/tren/opt/anaconda3/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n    - ''\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "mytxt='This is a cat. That is a dog.'\n",
    "print(nltk.sent_tokenize(mytxt))\n",
    "text=nltk.Text(nltk.word_tokenize(mytxt))\n",
    "text.plot()\n",
    "text4.dispersion_plot(['democracy','freedom','duties'])\n",
    "dist=nltk.FreqDist(text4)\n",
    "print(dist['freedom'])\n",
    "print([w for w in dist.keys() if dist[w]>1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2oOniVT-RLFp"
   },
   "source": [
    "### 2.3 Term Frequency–Inverse Document Frequency (TF-IDF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uHcx7YllRLFq"
   },
   "source": [
    "#### 2.3.1 Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "iJeBezMfRLFq"
   },
   "outputs": [],
   "source": [
    "import math\n",
    "def tf(word, count): #count是一個word count的dictionary\n",
    "    return count[word] / sum(count.values())\n",
    "def nDoc_have(word, count_list):\n",
    "    return sum(1 for count in count_list if word in count)\n",
    "def idf(word, count_list):\n",
    "    return math.log(len(count_list)) / (1 + nDoc_have(word, count_list))\n",
    "def tfidf(word, count, count_list):\n",
    "    return tf(word, count) * idf(word, count_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0PN1vnl_RLFr"
   },
   "source": [
    "#### 2.3.2 Testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "k6lmW8AcRLFr",
    "outputId": "96a31d28-3d66-44e8-af12-ffbd6b7cb2f2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'a': 2, 'b': 1, 'c': 1, 'd': 1}, {'a': 1, 'b': 2, 'c': 1, 'd': 1}, {'a': 1, 'b': 1, 'c': 2, 'd': 1}]\n"
     ]
    }
   ],
   "source": [
    "doc=['']*3\n",
    "doc[0]=['a','a','b','c','d']\n",
    "doc[1]=['a','b','b','c','d']\n",
    "doc[2]=['a','b','c','c','d']\n",
    "\n",
    "#construct word counts:\n",
    "count = [{}, {}, {}]\n",
    "\n",
    "for d in range(3):\n",
    "    for word in doc[d]:\n",
    "        if word not in count[d]:\n",
    "            count[d][word]=1\n",
    "        else:\n",
    "            count[d][word]+=1\n",
    "            \n",
    "print(count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lqBfBxiiRLFs"
   },
   "source": [
    "#### 2.3.3 TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "3Hf1rREVRLFs",
    "outputId": "b8842986-2a32-4816-a43a-981627ff4fc7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.10986122886681099\n"
     ]
    }
   ],
   "source": [
    "print(tfidf('a',count[0],count)) # \"a\" in the first document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "QSEtUGsKRLFs",
    "outputId": "c209bb92-5f85-475d-ec98-17f8f03c8d4d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top words in document 1\n",
      "\tWord: a, TF-IDF: 0.10986\n",
      "\tWord: b, TF-IDF: 0.05493\n",
      "\tWord: c, TF-IDF: 0.05493\n",
      "Top words in document 2\n",
      "\tWord: b, TF-IDF: 0.10986\n",
      "\tWord: a, TF-IDF: 0.05493\n",
      "\tWord: c, TF-IDF: 0.05493\n",
      "Top words in document 3\n",
      "\tWord: c, TF-IDF: 0.10986\n",
      "\tWord: a, TF-IDF: 0.05493\n",
      "\tWord: b, TF-IDF: 0.05493\n"
     ]
    }
   ],
   "source": [
    "#print the tf-idf of each word in each documetes\n",
    "keywords=[]\n",
    "for d in range(3):\n",
    "    print(\"Top words in document {}\".format(d+1))\n",
    "    scores={word: tfidf(word,count[d],count) for word in count[d]}\n",
    "    sorted_words=sorted(scores.items(),key=lambda x:x[1],reverse=True)\n",
    "    for word, score in sorted_words[:3]:\n",
    "        print(\"\\tWord: {}, TF-IDF: {}\".format(word,round(score,5)))\n",
    "        #create a list of keywords:\n",
    "        keywords+=word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5cbFj4WDRLFt"
   },
   "source": [
    "#### 2.3.4 TF-IDF-based document vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "bgja6xW3RLFt",
    "outputId": "d617aa69-1b91-45c0-fa04-f4febd2c6226"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['c', 'a', 'b']\n",
      "[[0.054930614433405495, 0.10986122886681099, 0.054930614433405495], [0.054930614433405495, 0.054930614433405495, 0.10986122886681099], [0.10986122886681099, 0.054930614433405495, 0.054930614433405495]]\n"
     ]
    }
   ],
   "source": [
    "keywords=list(set(keywords)) #remove duplicates\n",
    "print(keywords)\n",
    "\n",
    "#create a feature vector for each document:\n",
    "fv=['']*3\n",
    "for d in range(3):\n",
    "    idx=0 #feature index\n",
    "    fv[d]=[tfidf(word,count[d],count) for word in keywords]\n",
    "print(fv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-YOPRP78RLFu"
   },
   "source": [
    "### 2.4 Chinese \"word\" segementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "KF4aTZwQRLFu",
    "outputId": "64eaaaa2-8c7e-482b-9b2e-48d6ba7741a8"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Dumping model to file cache /var/folders/9p/gv8q6rsj23960sv_fdkvh0jm0000gn/T/jieba.cache\n",
      "Loading model cost 0.536 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "結巴|到底|會不會|成功|地|分解|這行|字|呢|?\n",
      "結|巴|到底|會|不|會|成功|地|分解|這|行|字|呢|?\n"
     ]
    }
   ],
   "source": [
    "#!pip install jieba\n",
    "import jieba\n",
    "text='結巴到底會不會成功地分解這行字呢?'\n",
    "wordlist=jieba.cut(text,cut_all=False)\n",
    "print(\"|\".join(wordlist))\n",
    "wordlist=jieba.cut(text,cut_all=True)\n",
    "print(\"|\".join(wordlist))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eBgjyerrRLFv"
   },
   "source": [
    "### 2.5 Topic Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BfeR6REVRLFv"
   },
   "source": [
    "#### 2.5.0 Word ID & Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "DvMrVC6JRLFv",
    "outputId": "537caae6-3b65-42ce-bee5-af0f35686c01"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary<253854 unique tokens: ['a', 'abacus', 'abilities', 'ability', 'able']...>\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "import gensim.downloader as api\n",
    "corpus=api.load('text8')\n",
    "dictionary=gensim.corpora.Dictionary(corpus) # generate a dictionary from the text corpus\n",
    "print(dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "ktUK5dnkRLFw",
    "outputId": "622a9a66-4a2d-4071-9d7e-0e505a3794e2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 1), (1, 1), (2, 1), (3, 1)]\n"
     ]
    }
   ],
   "source": [
    "print(dictionary.doc2bow(['a', 'abacus', 'abilities', 'ability']))\n",
    "corpus2=[dictionary.doc2bow(word) for word in corpus]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PJnsEH-6RLFw"
   },
   "source": [
    "#### 2.5.1 Latent Dirichlet Allocation (LDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "Y4nPToHiRLFx"
   },
   "outputs": [],
   "source": [
    "model=gensim.models.ldamodel.LdaModel(corpus2, num_topics=5, id2word=dictionary) #LDA training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "4ettMkeNRLFx",
    "outputId": "9bff5e3f-7306-420b-a67f-89a0b9998e69"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, '0.057*\"the\" + 0.035*\"of\" + 0.031*\"one\" + 0.024*\"and\" + 0.021*\"a\"'),\n",
       " (1, '0.062*\"the\" + 0.024*\"of\" + 0.023*\"a\" + 0.023*\"one\" + 0.023*\"and\"'),\n",
       " (2, '0.072*\"the\" + 0.035*\"of\" + 0.023*\"one\" + 0.021*\"to\" + 0.020*\"in\"'),\n",
       " (3, '0.054*\"the\" + 0.034*\"of\" + 0.029*\"one\" + 0.028*\"and\" + 0.022*\"in\"'),\n",
       " (4, '0.056*\"the\" + 0.043*\"of\" + 0.028*\"and\" + 0.025*\"in\" + 0.020*\"one\"')]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.print_topics(num_words=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yICGebirRLFx"
   },
   "source": [
    "## 3 Modern Natural Language Processing (NLP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YQBW71p2RLFy"
   },
   "source": [
    "### 3.1 Word2Vec\n",
    "Check <a href=\"https://github.com/RaRe-Technologies/gensim-data\">this</a> & <a href=\"https://github.com/3Top/word2vec-api#where-to-get-a-pretrained-models\">this</a> for more text datasets & pretained models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DRY1cTAHRLFy"
   },
   "source": [
    "#### 3.1.0 Train a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "jWUWkke6RLFy"
   },
   "outputs": [],
   "source": [
    "import gensim\n",
    "import gensim.downloader as api\n",
    "corpus=api.load('text8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "719eLk5MRLFz"
   },
   "outputs": [],
   "source": [
    "model = gensim.models.word2vec.Word2Vec(corpus) # training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "y9LW4tWLRLFz",
    "outputId": "b3a6f445-cff0-4e26-8aaa-d2f4d0256873"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('driver', 0.7849211692810059),\n",
       " ('motorcycle', 0.7434033751487732),\n",
       " ('cars', 0.7229942679405212),\n",
       " ('taxi', 0.7160823941230774),\n",
       " ('vehicle', 0.6912055611610413),\n",
       " ('truck', 0.686586320400238),\n",
       " ('cab', 0.6670401096343994),\n",
       " ('racing', 0.6656779646873474),\n",
       " ('passenger', 0.6519773006439209),\n",
       " ('sidecar', 0.6474007964134216)]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar(\"car\") # testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rNQFqQNKRLFz"
   },
   "source": [
    "#### 3.1.1 Man:King :: Woman:?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "9XcMiz_fRLFz",
    "outputId": "02fac641-8c88-4756-e972-299fb3e76ab8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('queen', 0.6857359409332275),\n",
       " ('prince', 0.6322767734527588),\n",
       " ('princess', 0.6159293055534363),\n",
       " ('empress', 0.6010196805000305),\n",
       " ('throne', 0.6002517342567444),\n",
       " ('regent', 0.5805041193962097),\n",
       " ('elizabeth', 0.5800679922103882),\n",
       " ('kings', 0.5714446902275085),\n",
       " ('emperor', 0.5714309811592102),\n",
       " ('mary', 0.5664592385292053)]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar(positive=['woman', 'king'], negative=['man'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "24JotAD6RLF0"
   },
   "source": [
    "#### 2.6.2 Man:Doctor :: Woman:?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "5WBbYnYMRLF0",
    "outputId": "5330d771-0a5f-4d01-d212-661a08270534"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('child', 0.5809516310691833),\n",
       " ('teacher', 0.5554264187812805),\n",
       " ('nurse', 0.5533174872398376),\n",
       " ('murderer', 0.5325186848640442),\n",
       " ('detective', 0.5162506103515625),\n",
       " ('lesbian', 0.5101373791694641),\n",
       " ('priestess', 0.5079809427261353),\n",
       " ('prostitute', 0.5074547529220581),\n",
       " ('helen', 0.5060341358184814),\n",
       " ('herself', 0.49865785241127014)]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar(positive=['woman', 'doctor'], negative=['man'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aEbzUhe1RLF0"
   },
   "source": [
    "#### 2.6.3 (Good + Best)/2 = Better ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "hvuD32HZRLF1",
    "outputId": "514da8f5-66e1-477e-c18b-1b228a696a80"
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"Key 'Good' not present\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-33-16e4ad0fe71d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_vec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Good'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/gensim/utils.py\u001b[0m in \u001b[0;36mnew_func1\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1520\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1521\u001b[0m                 )\n\u001b[0;32m-> 1522\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1523\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1524\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mnew_func1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36mword_vec\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    458\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mword_vec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    459\u001b[0m         \u001b[0;34m\"\"\"Compatibility alias for get_vector(); must exist so subclass calls reach subclass get_vector().\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 460\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    461\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    462\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_mean_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpre_normalize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpost_normalize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_missing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36mget_vector\u001b[0;34m(self, key, norm)\u001b[0m\n\u001b[1;32m    445\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    446\u001b[0m         \"\"\"\n\u001b[0;32m--> 447\u001b[0;31m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    448\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mnorm\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    449\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfill_norms\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36mget_index\u001b[0;34m(self, key, default)\u001b[0m\n\u001b[1;32m    419\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mdefault\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    420\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 421\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Key '{key}' not present\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    422\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    423\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnorm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: \"Key 'Good' not present\""
     ]
    }
   ],
   "source": [
    "model.wv.word_vec('Good')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "JARtygm_RLF1"
   },
   "outputs": [],
   "source": [
    "what=(model.wv.word_vec('good')+model.wv.word_vec('best'))/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "ER39HStnRLF1",
    "outputId": "f9d90176-fd8b-421e-a11c-e5c481b96f94"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('best', 0.8559709787368774),\n",
       " ('good', 0.7881002426147461),\n",
       " ('better', 0.6068601608276367),\n",
       " ('bad', 0.5423668026924133),\n",
       " ('fun', 0.5169402360916138),\n",
       " ('poor', 0.4934327006340027),\n",
       " ('little', 0.49075865745544434),\n",
       " ('helpful', 0.4829798936843872),\n",
       " ('finest', 0.47662466764450073),\n",
       " ('what', 0.47124817967414856)]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.similar_by_vector(what)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "K3O8lxodRLF2",
    "outputId": "b320330c-3487-4336-97d2-da64d8aa558c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('better', 0.588021993637085),\n",
       " ('bad', 0.5557962656021118),\n",
       " ('fair', 0.5461308360099792),\n",
       " ('fun', 0.5098727941513062),\n",
       " ('little', 0.4782398045063019),\n",
       " ('leisure', 0.4754738211631775),\n",
       " ('greatest', 0.4739540219306946),\n",
       " ('worst', 0.46178722381591797),\n",
       " ('excellent', 0.4616394340991974),\n",
       " ('practical', 0.4583337903022766)]"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar(positive=['good', 'best'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H735xwD2RLF2"
   },
   "source": [
    "### 3.1 Sentence/Document Vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "bVK9dTSxRLF2"
   },
   "outputs": [],
   "source": [
    "# !pip install spacy or conda install -c conda-forge spacy for M1 Macs\n",
    "# !python -m spacy download en_core_web_sm\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "o-BigQZsRLF3"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'en_core_web_sm'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-2745dba7816f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Load English tokenizer, tagger, parser and NER\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0men_core_web_sm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mnlp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0men_core_web_sm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Process whole documents\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'en_core_web_sm'"
     ]
    }
   ],
   "source": [
    "# Load English tokenizer, tagger, parser and NER\n",
    "import en_core_web_sm\n",
    "nlp = en_core_web_sm.load()\n",
    "\n",
    "# Process whole documents\n",
    "doc1 = nlp(\"I love you!\")\n",
    "doc2 = nlp(\"I like you!\")\n",
    "doc3 = nlp(\"I hate you!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4sp3q8USRLF3",
    "outputId": "0715afa2-5226-4783-ecd2-2b6982a60e09"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(96,)\n",
      "[-0.09983218  0.35353643  0.5265776  -2.0017223   1.4662154  -0.33170778\n",
      "  1.0827445   0.74185747  0.9684267   0.15458488 -0.83901423 -0.31761074\n",
      "  1.1670883   1.6654768  -0.30134588 -1.393422   -0.5176586  -0.44548252\n",
      "  0.64707303 -0.56519    -0.09047961 -0.969736   -0.44712326 -0.91941214\n",
      " -1.680759   -0.18274733  0.32576013 -2.6253097  -0.12723407  1.2189727\n",
      "  0.46241376 -0.96291625  2.1276584   1.8101883   1.7117918  -2.8340235\n",
      " -0.4939808  -0.21001339  0.29979473 -1.0554987  -0.12981123  0.0149786\n",
      "  0.43961585 -1.8888178   1.8597324  -0.53544307  1.5854046  -0.6399481\n",
      " -2.6180916   1.1852651  -1.167548   -0.86542356  0.54269934 -0.6894389\n",
      " -1.5810661  -1.0903887  -0.35865986  0.41884246 -0.2837396   2.520783\n",
      "  0.6547807  -0.6294886   0.58366    -0.8731143  -1.3666099  -0.71271324\n",
      " -0.6313154  -0.38491994  0.41647953  0.07151747 -0.38954067  1.3536111\n",
      " -0.02793354  0.21484032 -0.19864672 -1.3100643   1.9951842   0.8023096\n",
      "  1.7919707  -0.4781978  -0.8473945   0.06738667 -0.14452577  1.8957655\n",
      "  2.0533586   1.7801816   1.2883544   0.67228115  0.00673186 -0.04924235\n",
      "  2.0610952   0.5236438  -1.792839    1.0195714  -0.6732619   0.9118332 ]\n"
     ]
    }
   ],
   "source": [
    "print(doc1.vector.shape)\n",
    "print(doc1.vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "r_J2o-VvRLF3",
    "outputId": "3e4f628e-f67a-49d9-e97e-d7721e0c3dfe"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7934198520902852\n",
      "0.9119906074696802\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.7/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Doc.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
      "  \"__main__\", mod_spec)\n",
      "/usr/lib/python3.7/runpy.py:193: ModelsWarning: [W007] The model you're using has no word vectors loaded, so the result of the Doc.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
      "  \"__main__\", mod_spec)\n"
     ]
    }
   ],
   "source": [
    "print(doc1.similarity(doc2))\n",
    "print(doc1.similarity(doc3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2vzzBsdERLF4"
   },
   "source": [
    "## 4 ChatBots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "83cYWYATRLF5"
   },
   "source": [
    "### 4.2 BackEnd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B5H5D5FMRLF5"
   },
   "source": [
    "#### 4.2.1 ChatterBot: Write your own QAs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "VdpwdHgaRLF6"
   },
   "outputs": [],
   "source": [
    "#!pip install chatterbot chatterbot_corpus\n",
    "#!python -m spacy link en_core_web_sm en\n",
    "import chatterbot\n",
    "from chatterbot import ChatBot\n",
    "from chatterbot.trainers import ListTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ont0Dky6RLF6",
    "outputId": "62c2ee5b-a007-43b2-c44f-ea37393a3f01"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "List Trainer: [####################] 100%\n"
     ]
    }
   ],
   "source": [
    "chatbot = ChatBot(\"Psychoinformatics\")\n",
    "\n",
    "conversation = [\n",
    "    \"Hello\",\n",
    "    \"Hi there!\",\n",
    "    \"How are you doing?\",\n",
    "    \"I'm doing great.\",\n",
    "    \"That is good to hear\",\n",
    "    \"Thank you.\",\n",
    "    \"You're welcome.\",\n",
    "    \"帥喔!\",\n",
    "    \"帥只有一個字，卻跟了我一輩子~\"\n",
    "]\n",
    "\n",
    "trainer = ListTrainer(chatbot)\n",
    "\n",
    "trainer.train(conversation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bR6UBv_zRLF6",
    "outputId": "8e076abd-52ba-4d96-f34f-202c468d2bf7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How are you doing?\n",
      "帥只有一個字，卻跟了我一輩子~\n"
     ]
    }
   ],
   "source": [
    "print(chatbot.get_response(\"Hi there\"))\n",
    "print(chatbot.get_response(\"帥喔!\"))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "uHcx7YllRLFq",
    "0PN1vnl_RLFr",
    "lqBfBxiiRLFs",
    "5cbFj4WDRLFt",
    "BfeR6REVRLFv",
    "PJnsEH-6RLFw",
    "DRY1cTAHRLFy",
    "rNQFqQNKRLFz",
    "24JotAD6RLF0",
    "aEbzUhe1RLF0"
   ],
   "name": "13_examples.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
